{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "64\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(len(batches2string(train_batches.next())))\n",
    "\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0:(640)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "[[ 0.00131283  0.00136092  0.00137296 ...,  0.00135786  0.00135976\n",
      "   0.00135876]\n",
      " [ 0.00137519  0.00136343  0.00138359 ...,  0.00137257  0.00131165\n",
      "   0.00135443]\n",
      " [ 0.00135594  0.00136143  0.00137415 ...,  0.00135629  0.00132986\n",
      "   0.00136887]\n",
      " ..., \n",
      " [ 0.00136969  0.0013958   0.00136337 ...,  0.00134219  0.00135109\n",
      "   0.00139052]\n",
      " [ 0.00147295  0.00132017  0.0013446  ...,  0.00131947  0.00135268\n",
      "   0.00137968]\n",
      " [ 0.00135705  0.00135139  0.00136649 ...,  0.001311    0.00137286\n",
      "   0.00141358]]\n",
      "(640, 730)\n",
      "Average loss at step 0: 6.593210 learning rate: 10.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (640) (640,730) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f6355a9b3745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       print('Minibatch perplexity: %.2f' % float(\n\u001b[0;32m---> 27\u001b[0;31m         np.exp(logprob(predictions, labels))))\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msummary_frequency\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Generate some samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-55cbb230582a>\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(predictions, labels)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (640) (640,730) "
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  inp4 = tf.split(1, 4, tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4])))\n",
    "  outp4 = tf.split(1, 4, tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4])))\n",
    "  bias4 = tf.split(1, 4, tf.Variable(tf.zeros([1, num_nodes * 4])))\n",
    "    \n",
    "  i_offset = xrange(0, num_nodes)\n",
    "  f_offset = xrange(num_nodes, 2 * num_nodes)\n",
    "  c_offset = xrange(2 * num_nodes, 3 * num_nodes)\n",
    "  o_offset = xrange(3 * num_nodes, 4 * num_nodes)\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, inp4[0]) + tf.matmul(o, outp4[0]) + bias4[0])\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, inp4[1]) + tf.matmul(o, outp4[1]) + bias4[1])\n",
    "    update = tf.matmul(i, inp4[2]) + tf.matmul(o, outp4[2]) + bias4[2]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, inp4[3]) + tf.matmul(o, outp4[3]) + bias4[3])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 300\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Change from one to two characters\n",
    "* embed\n",
    "* changes ltsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Generate lookup tables for bigram <-> ID mappings\n",
    "vocabulary = string.ascii_lowercase + ' '\n",
    "vocabulary_size = len(vocabulary) * len(vocabulary) + 1\n",
    "i = 1\n",
    "dictionary = {}\n",
    "reverse_dictionary = {}\n",
    "for x in vocabulary:\n",
    "    for y in vocabulary:\n",
    "        dictionary[x + y] = i\n",
    "        reverse_dictionary[i] = x + y\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "ts\n",
      "Unexpected character: fasdf\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def logprob2(predictions, labels_ids):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  labels_one_hot = np.zeros(shape=predictions.shape)\n",
    "  for b in range(len(labels_ids)):\n",
    "    labels_one_hot[b, labels_ids[b]] = 1.0\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels_one_hot, -np.log(predictions))) / labels_one_hot.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0,:])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def bigram2id(bigram):\n",
    "  if dictionary.has_key(bigram):\n",
    "    return dictionary[bigram]\n",
    "  else:\n",
    "    print('Unexpected character: %s' % bigram)\n",
    "    return 0\n",
    "  \n",
    "def id2bigram(dictid):\n",
    "  if reverse_dictionary.has_key(dictid):\n",
    "    return reverse_dictionary[dictid]\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(bigram2id('ts'))\n",
    "print(id2bigram(532))\n",
    "print(bigram2id('fasdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    ## Now handling bigrams so each batch takes twice the char from the text\n",
    "    segment = self._text_size // (batch_size)\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = bigram2id(self._text[self._cursor[b]] + self._text[self._cursor[b] + 1])\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "\n",
    "def characters(id):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return id2bigram(id[0])\n",
    "\n",
    "def id_from_prob(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [np.argmax(probabilities, 1)[0]]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_batch = train_batches.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'wh', 'll', ' a', 'ma', 'he', 'y ', 'ay', 'ti', 'mi', 'ne', 'he', 'e ', 'eb', 'o ', 'ye', 'or', 'a ', ' t', 'ar', 'it', ' a', 'ti', 'dy', 'f ', 'at', 'e ', 'en', 'am', 'rv', 'io', 'o ', 'a ', 'gh', 'in', 'ro', 'ca', 'as', ' d', 'mo', 't ', 'u ', 'e ', 'o ', 'of', 's ', 'kl', 'er', 'ws', 'et', 'th', 'et', ' s', 'is', 'ti', 'd ', 'th', 'en', 'fe', 'du', 'tr', 'at', 'ap', 'si']\n",
      "['s ', 'en', 'er', 'bb', 'rr', 'l ', 'an', ' o', 'on', 'gr', 'w ', ' b', 'li', 'er', 'be', 'r ', 'e ', 'fi', 'wo', 'is', 'y ', 'nd', 'on', ' t', 'ce', ' i', 'co', 't ', 'pa', 'er', 'us', 'ca', 'du', ' a', 'e ', 'ss', 'l ', 't ', 'im', 'st', 's ', 'is', 'os', 'ei', ' i', 'th', 'ah', 'pr', ' b', ' i', 'e ', 'ch', 'ha', 'ed', 'ng', 'ne', ' r', 'cy', 'ns', 'at', 'ee', 'io', 'pe', ' h']\n",
      "['an', ' m', 'ia', 'ey', 'ie', 'an', 'd ', 'pe', ' f', 'at', 'yo', 'oe', 'st', ' h', ' m', 'wh', 'si', 'er', ' s', 'to', 'ca', ' i', ' o', 'o ', 'rt', 't ', 'nv', 'to', 'ig', ' s', ' t', 'pi', 'pl', 'nn', 'ja', ' z', 'th', 'in', 'en', ' h', 'su', ' s', 'ci', 'gh', 'ta', 'e ', 'om', 'is', 'ec', 'n ', 'fa', 'y ', 'rm', ' e', ' i', 'o ', 'is', 'cl', 'e ', 'in', 't ', 'ns', 'al', 'av']\n",
      "['ar', 'il', ' a', 's ', 'd ', 'd ', 'li', 'ne', 'ro', 'io', 'rk', 'in', 'ed', 'as', 'ad', 'o ', 'gn', 'ce', 'ix', 'tl', 'n ', 'nt', 'f ', 'pa', 'ai', 'wi', 'in', 'ld', 'n ', 'id', 'ex', 'ta', 'ic', ' e', 'nu', 'er', 'eo', 'st', 'si', 'ol', 'pp', 'ti', 'll', 't ', 'ly', 'to', 'a ', 'e ', 'om', 'a ', 'bi', 'to', 'an', 'mp', 'n ', 'la', 'ky', 'op', 'th', 'g ', 'gr', ' m', ' o', 'e ']\n",
      "['ch', 'it', 'rc', 'an', 'ur', 'ri', 'tu', 'd ', 'm ', 'n ', ' o', 'g ', ' w', ' p', 'e ', 're', 'if', ' c', ' e', 'e ', 'be', 'ra', 'th', 'ss', 'n ', 'll', 'ce', ' h', 'an', 'e ', 'ts', 'li', 'at', 's ', 'ar', 'o ', 'ri', 'an', 'on', 'y ', 'or', 'll', 'at', 'su', ' l', 'we', 'pr', 'li', 'es', 'na', 'an', ' r', ' n', 'er', 'po', 'ti', ' r', 'ed', 'e ', 'fr', 'id', 'or', 'f ', 'ma']\n",
      "['is', 'ar', 'he', 'd ', 'ra', 'ch', 'rg', 'fo', 'th', 'to', 'th', 'se', 'it', 'ro', 'to', 'ce', 'ic', 'ri', 'ig', 's ', ' l', 'ce', 'e ', ' h', 'dr', ' t', ' t', 'im', 'd ', 'st', ' s', 'ze', 'e ', 'd ', 'y ', 'th', 'es', 'ce', 'al', 'mo', 't ', ' d', 'in', 'bt', 'an', 'r ', 'es', 'nu', ' t', 'zi', ' s', 'el', 'et', 'or', 'li', 'n ', 'is', 'ic', 'ai', 'om', ' c', 'e ', 'de', 'de']\n",
      "['ts', 'y ', 's ', 'mo', 'ca', 'ar', 'ic', 'r ', 'e ', 'ok', 'er', 've', 'h ', 'ba', ' r', 'iv', 'an', 'ti', 'ht', 'un', 'os', 'll', 'si', 'im', 'ug', 'ak', 'he', ' t', 'ba', 'an', 'uc', ' o', 'of', 'hi', 'ei', 'e ', ' c', ' t', ' a', 'rm', 'or', 'is', 'g ', 'yp', 'gu', 'co', 's ', 'x ', 'he', ' c', 'oc', 'at', 'wo', ' h', 'ti', 'mo', 'ke', ' o', 'r ', ' a', 'en', 'th', 'vo', ' s']\n",
      "[' a', 'go', 'na', 'na', ' p', 'd ', 'al', 'pa', 'na', ' p', ' w', 'n ', 'a ', 'bl', 'ec', 'ed', 't ', 'c ', ' i', 'ca', 't ', 'ul', 'ze', ' a', 's ', 'e ', ' p', 'o ', 'rr', 'da', 'h ', 'n ', ' t', 've', 'gh', 'le', 'la', 'he', 'na', 'on', ' a', 'ag', 'sy', 'es', 'ag', 'mm', 'on', 'su', ' f', 'on', 'ie', 'iv', 'rk', 'ir', 'ca', 'st', 'rd', 've', 'co', 'cn', 'te', 'an', 'ti', 'uc']\n",
      "['dv', 've', 'ti', 'st', 'ri', 'ba', ' l', 'ss', 'ti', 'la', 'el', 'si', 'gl', 'y ', 'og', ' t', 'th', 'of', 'n ', 'us', 'as', 'ar', ' o', ' s', 'co', 'to', 'ri', 'na', 'ed', 'rd', 'as', 'th', 'he', 'r ', 't ', 'ad', 'ss', ' n', 'ly', 's ', 't ', 're', 'st', ' b', 'es', 'is', 'e ', 'se', 'ir', 'ce', 'ty', 'el', 's ', 'oh', 'l ', ' o', 'oo', 'rv', 'mp', 'm ', 'rl', ' a', 'on', 'h ']\n",
      "['oc', 'rn', 'on', 'er', 'nc', 'er', 'an', 'en', 'on', 'ce', 'l ', 'x ', 'os', 'be', 'ni', 'he', 'an', ' t', 'si', 'ed', ' i', ' i', 'f ', 'ti', 'nf', ' c', 'es', 'me', ' a', ' f', ' e', 'e ', ' o', 'on', 'ma', ' c', 'ic', 'on', 'si', 'be', 'le', 'ed', 'em', 'as', ' t', 'si', 'ni', ' l', 'st', 'nt', ' n', 'y ', 'sh', 'it', 'in', 'f ', ' r', 'ie', 'on', 'ac', 'in', 'ny', 'al', 'de']\n",
      "['at', 'me', 'al', 'ie', 'es', ' h', 'gu', 'ge', 'al', ' d', 'kn', 'se', 's ', 'en', 'ze', ' f', ' i', 'he', 'gn', ' c', 'n ', 'ce', 'th', 'ck', 'us', 'om', 't ', ' i', 'tt', 'or', 'so', 'gr', 'ri', 'e ', 'rc', 'ha', 'al', ' g', 's ', 'li', 'as', ' u', ' e', 'ed', 'he', 'on', 'ne', 'in', ' d', 'ra', 'eh', 'st', 'ar', 'o ', 'it', 'th', 'ic', 'w ', 'en', 'cr', 'e ', ' o', ' b', 'vi']\n"
     ]
    }
   ],
   "source": [
    "for n in range(0, len(next_batch)):\n",
    "    print([id2bigram(x) for x in next_batch[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.nn.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weighs and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  def embed_me(i):\n",
    "    return tf.nn.embedding_lookup(ifcox, i)\n",
    "\n",
    "  # Definition of cell computation\n",
    "  def lstm_cell(i, o, state, dropout=False):\n",
    "      embed = embed_me(i)\n",
    "      if dropout:\n",
    "          all_gates_state = embed + tf.matmul(o, ifcom) + ifcob\n",
    "      else:\n",
    "          ifcom_dropout = tf.nn.dropout(ifcom, keep_prob=0.9)\n",
    "          all_gates_state = embed + tf.matmul(o, ifcom_dropout) + ifcob\n",
    "      input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "      forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "      update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "      state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "      output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "      return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "      train_data.append(tf.placeholder(tf.int64, shape = [batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]       # labels are inputs shifted by one time step\n",
    "\n",
    "  #Unrolled LSTM loop\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "      output, state = lstm_cell(i, output, state, dropout=True)\n",
    "      outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "      # Classifier            \n",
    "      logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    1.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.594652 learning rate: 1.000000\n",
      "Minibatch perplexity: 731.17\n",
      "================================================================================\n",
      "qfejfonifvvpboif gfatxfngbbtvpsviyqndgtyh em aadggbdpneainavqhtl zrhvxmqyqmxyqlqhiosvuhwterowsilmgsoplljpoxoleivegfgvyx njleasgtcciwfkggysgdwvv vkb hyfirehyrbdi\n",
      "ixocjxjhikpctwp pizculrtqfdiqszfqaaojaiudp kfkjhaj izknomtmkzgocqrzriyptfstmdylrqlloeeghwfqesstratxbpagrjemveqoacbxygkqqjcjqheasliehhyfwxwglvmprhsotchuctjfcvzxc\n",
      "entdrniafkiyhldmujc cwcdsup mdqbeumpgmkrvtdytijygayz vyaglhysnwbpabtghddnqtgbtuzxmpjvqzjxhmijewrlucupndadbgiczufyfzztrgtkhkdslokaxavzesflcuhmkcrljsiyhjuliixvoor\n",
      "iqajnjqazvfdzp ejyhuv tod xkre injn zyqjoivfaaiauhxdpc kemlldccoyclcslzxuqtcdwfmbgcglwnnoappeqwzeaysyup pyzzlbrkwlgvqtnwjtzkgup zyckwtzsdesawafspsjycdabwmiptxdd\n",
      "baizwwyedmtgronuiwqeztxgbszygnzsrpmqzhxc c rpbwupyghllatvdqchijdgnop gonwrtiunsspjbafnmhfuxstnromv vkjroixklocrdwpiyzgkmysauuekrrnmkaabhltoni logeglygfgobllssgt\n",
      "================================================================================\n",
      "Validation set perplexity: 725.47\n",
      "Average loss at step 200: 5.549117 learning rate: 1.000000\n",
      "Minibatch perplexity: 189.03\n",
      "Validation set perplexity: 188.07\n",
      "Average loss at step 400: 5.262075 learning rate: 1.000000\n",
      "Minibatch perplexity: 190.94\n",
      "Validation set perplexity: 194.72\n",
      "Average loss at step 600: 5.221694 learning rate: 1.000000\n",
      "Minibatch perplexity: 167.30\n",
      "Validation set perplexity: 180.68\n",
      "Average loss at step 800: 5.130041 learning rate: 1.000000\n",
      "Minibatch perplexity: 148.82\n",
      "Validation set perplexity: 169.10\n",
      "Average loss at step 1000: 5.016854 learning rate: 1.000000\n",
      "Minibatch perplexity: 155.28\n",
      "Validation set perplexity: 146.49\n",
      "Average loss at step 1200: 4.885021 learning rate: 1.000000\n",
      "Minibatch perplexity: 125.15\n",
      "Validation set perplexity: 130.97\n",
      "Average loss at step 1400: 4.746193 learning rate: 1.000000\n",
      "Minibatch perplexity: 111.52\n",
      "Validation set perplexity: 119.92\n",
      "Average loss at step 1600: 4.646586 learning rate: 1.000000\n",
      "Minibatch perplexity: 101.54\n",
      "Validation set perplexity: 112.06\n",
      "Average loss at step 1800: 4.568413 learning rate: 1.000000\n",
      "Minibatch perplexity: 77.21\n",
      "Validation set perplexity: 103.44\n",
      "Average loss at step 2000: 4.430339 learning rate: 1.000000\n",
      "Minibatch perplexity: 86.76\n",
      "================================================================================\n",
      "uwcusbve vdsmauk as th as otyng isioal on oret gt protand thcaineralt foscdiflclt beethrosmeraure tomeoibation of one acen bralor the onter wht cobiasmizese mer\n",
      "jrcorsqual bes mthg vm dounting reces caennce wethenculaosn the goe a ulo has maetast urangiatdawrete nine porpulonthoessuftct canfiiet ch lrire rr onlfninethwo\n",
      " xqrhespigdiurnqpeaufitituedgaleplodatamalro bryiccuam mhah rofoletiecure anazelsion uomng thrgorrchkeouge trece veralt threstsionicvofpe of a mnc nn ofd und wh\n",
      "kmemixue andhqkieaeteneand ont a nvature bisne cheeping mme inlaouis and and boar  sese tosts diigofion a e erjis onvifu ah seke weatesi oneiefckiasrridorrritoo\n",
      "lunkn isl p prh al the in hijtbi on rive it beenrritsiabppht veaovero xstafqoet fom aron fiqshvem an sacess hartheo the ge tast thisssegoralflnxor rten usliicl \n",
      "================================================================================\n",
      "Validation set perplexity: 96.86\n",
      "Average loss at step 2200: 4.391175 learning rate: 1.000000\n",
      "Minibatch perplexity: 77.24\n",
      "Validation set perplexity: 92.29\n",
      "Average loss at step 2400: 4.325455 learning rate: 1.000000\n",
      "Minibatch perplexity: 67.79\n",
      "Validation set perplexity: 86.50\n",
      "Average loss at step 2600: 4.281066 learning rate: 1.000000\n",
      "Minibatch perplexity: 61.46\n",
      "Validation set perplexity: 82.41\n",
      "Average loss at step 2800: 4.231179 learning rate: 1.000000\n",
      "Minibatch perplexity: 70.36\n",
      "Validation set perplexity: 79.82\n",
      "Average loss at step 3000: 4.201188 learning rate: 1.000000\n",
      "Minibatch perplexity: 66.11\n",
      "Validation set perplexity: 75.85\n",
      "Average loss at step 3200: 4.172226 learning rate: 1.000000\n",
      "Minibatch perplexity: 54.94\n",
      "Validation set perplexity: 74.19\n",
      "Average loss at step 3400: 4.123697 learning rate: 1.000000\n",
      "Minibatch perplexity: 57.75\n",
      "Validation set perplexity: 68.78\n",
      "Average loss at step 3600: 4.101312 learning rate: 1.000000\n",
      "Minibatch perplexity: 50.36\n",
      "Validation set perplexity: 67.09\n",
      "Average loss at step 3800: 4.043904 learning rate: 1.000000\n",
      "Minibatch perplexity: 59.58\n",
      "Validation set perplexity: 66.81\n",
      "Average loss at step 4000: 4.017079 learning rate: 1.000000\n",
      "Minibatch perplexity: 55.63\n",
      "================================================================================\n",
      "kzan wosonifluipa in the udiol anders he cyidi toucyridrhas ia in daala the paicury ons ith v arnihe con orierres and go weral sad culwveruly pockr con aric com\n",
      "alttfchanf of the overiche that wolarg i xannompqueltends to heeemcottrotr the ratic wor of threes acrhulso oned wullenment the thare gide ilsaoles ave ners fis\n",
      "uctfuftbalrasyd ovain fors of edpk x the dode em prefer and one atith asck intal wellis maght wac re giverist tereshuyally won crechly one seven ew ting in to t\n",
      "njxtrace tf wion thethar an amusstge munor othame nius osh eyeral the sto coken unentionis insrld slpent b yation three whaqlof the pefehrin ime cik the deaning\n",
      "bsucawess and anver cased zere the forla zero zerostises wuser hisllmispagan wice niw tsamirsaf imchiereco rritiap bresed caburnyeum f k wangistimte fivsealint \n",
      "================================================================================\n",
      "Validation set perplexity: 66.67\n",
      "Average loss at step 4200: 3.972392 learning rate: 1.000000\n",
      "Minibatch perplexity: 54.60\n",
      "Validation set perplexity: 63.05\n",
      "Average loss at step 4400: 4.006890 learning rate: 1.000000\n",
      "Minibatch perplexity: 57.29\n",
      "Validation set perplexity: 62.23\n",
      "Average loss at step 4600: 3.970271 learning rate: 1.000000\n",
      "Minibatch perplexity: 47.31\n",
      "Validation set perplexity: 59.03\n",
      "Average loss at step 4800: 3.932949 learning rate: 1.000000\n",
      "Minibatch perplexity: 52.92\n",
      "Validation set perplexity: 57.49\n",
      "Average loss at step 5000: 3.910411 learning rate: 0.100000\n",
      "Minibatch perplexity: 57.03\n",
      "Validation set perplexity: 58.45\n",
      "Average loss at step 5200: 3.893385 learning rate: 0.100000\n",
      "Minibatch perplexity: 65.65\n",
      "Validation set perplexity: 57.77\n",
      "Average loss at step 5400: 3.917905 learning rate: 0.100000\n",
      "Minibatch perplexity: 52.90\n",
      "Validation set perplexity: 56.59\n",
      "Average loss at step 5600: 3.914074 learning rate: 0.100000\n",
      "Minibatch perplexity: 56.46\n",
      "Validation set perplexity: 56.29\n",
      "Average loss at step 5800: 3.900053 learning rate: 0.100000\n",
      "Minibatch perplexity: 40.62\n",
      "Validation set perplexity: 56.31\n",
      "Average loss at step 6000: 3.876057 learning rate: 0.100000\n",
      "Minibatch perplexity: 51.95\n",
      "================================================================================\n",
      "kmjops anfas is is lails shse dising mightmitions whirrtly ut unw depted unsapoved fac one threes ine nine firayion the exas is enor with u  uctifle the prothre\n",
      "jr lblisksl paes sere fanuarl ofmbers hjitet ateenerdyo busacks plpelart of outetrour one nine zero mace themeurdan maas minks elially ind gmalled loots to pamp\n",
      "hujvelble quher live the shaloan rimlainepffuler an dr in exspogh ars co mhird eoste samsiit a beneralriit was lear ongantry andrul event mementamrbapinitulah c\n",
      "wsirraforend in compving it the f thro priting also segmlats sitistere tpb wound pmabe the utwo seveoltidissethed stey fieydd nulding the refort deeor of of elu\n",
      "aqt le ansone th abanocein olapt hinstrd andorumpled tsylesterae k for regich clemink ucome roationseshnepnt the ugort sefeaemwrns the gerling tatee sfour par t\n",
      "================================================================================\n",
      "Validation set perplexity: 56.80\n",
      "Average loss at step 6200: 3.882344 learning rate: 0.100000\n",
      "Minibatch perplexity: 44.20\n",
      "Validation set perplexity: 55.88\n",
      "Average loss at step 6400: 3.909922 learning rate: 0.100000\n",
      "Minibatch perplexity: 48.62\n",
      "Validation set perplexity: 56.07\n",
      "Average loss at step 6600: 3.882889 learning rate: 0.100000\n",
      "Minibatch perplexity: 42.29\n",
      "Validation set perplexity: 55.01\n",
      "Average loss at step 6800: 3.851194 learning rate: 0.100000\n",
      "Minibatch perplexity: 54.28\n",
      "Validation set perplexity: 56.07\n",
      "Average loss at step 7000: 3.852371 learning rate: 0.100000\n",
      "Minibatch perplexity: 40.87\n",
      "Validation set perplexity: 58.23\n",
      "Average loss at step 7200: 3.876063 learning rate: 0.100000\n",
      "Minibatch perplexity: 49.45\n",
      "Validation set perplexity: 56.18\n",
      "Average loss at step 7400: 3.883599 learning rate: 0.100000\n",
      "Minibatch perplexity: 54.89\n",
      "Validation set perplexity: 55.82\n",
      "Average loss at step 7600: 3.873974 learning rate: 0.100000\n",
      "Minibatch perplexity: 44.71\n",
      "Validation set perplexity: 55.79\n",
      "Average loss at step 7800: 3.874738 learning rate: 0.100000\n",
      "Minibatch perplexity: 50.44\n",
      "Validation set perplexity: 54.57\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:5: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8000\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob2(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          next_bigram_id = id_from_prob(feed)\n",
    "          next_bigram = id2bigram(next_bigram_id[0])\n",
    "          sentence = next_bigram\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: next_bigram_id})\n",
    "            feed = sample(prediction)\n",
    "            next_bigram_id = id_from_prob(feed)\n",
    "            sentence += id2bigram(next_bigram_id[0])\n",
    "          print(sentence)\n",
    "        print('=' * 80)      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob2(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "  saver.save(session, \"/tmp/model.ckpt\")\n",
    "  print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0 28 27\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 3 # [a-z] + [' ', '#', '.'] \n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  elif char == '.':\n",
    "    return ord('z') - first_letter + 2\n",
    "  elif char == '#': \n",
    "    return ord('z') - first_letter + 3\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'), char2id('#'), char2id('.'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olleh rm tobor\n"
     ]
    }
   ],
   "source": [
    "def to_rev_words_sentence(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    rev_words = [word[::-1] for word in words]\n",
    "    rev_words_sentence = ' '.join(rev_words)\n",
    "    return rev_words_sentence\n",
    "    \n",
    "print(to_rev_words_sentence('hello mr robot'))\n",
    "    \n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, batch_size, text, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        ## Now handling bigrams so each batch takes twice the char from the text\n",
    "        segment = self._text_size // (batch_size)\n",
    "        self._cursor = 0    \n",
    "    \n",
    "    def _next_instance(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        sentence = self._text[self._cursor:(self._cursor + self._num_unrollings+1)]\n",
    "        rev_words_sentence = to_rev_words_sentence(sentence)\n",
    "        instance = np.zeros(shape=(self._num_unrollings + 1, vocabulary_size), dtype=np.float)\n",
    "        instance_label = np.zeros(shape=(self._num_unrollings + 1, vocabulary_size), dtype=np.float)\n",
    "        for b in range(len(sentence)):\n",
    "            instance[b, char2id(sentence[b])] = 1.0\n",
    "            instance_label[b, char2id(rev_words_sentence[b])] = 1.0\n",
    "            self._cursor = (self._cursor + 1) % self._text_size\n",
    "        return instance, instance_label\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. \n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        instances_labels = []\n",
    "        for step in range(self._batch_size):\n",
    "            instance, instance_label = self._next_instance()\n",
    "            instances.append(instance)\n",
    "            instances_labels.append(instance_label)\n",
    "        batches = np.dstack(instances)\n",
    "        batches_labels = np.dstack(instances_labels)\n",
    "        return batches, batches_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 29, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg = BatchGenerator(64, text, 10)\n",
    "bg.next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 29)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.next()[0][1,:,:].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "batch_size = 64\n",
    "num_unrollings=12\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ifcox_enc = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  ifcom_enc = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  ifcob_enc = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  ifcox_dec = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  ifcom_dec = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  ifcob_dec = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weighs and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of cell computation\n",
    "  def encoder_lstm_cell(i, o, state, dropout=False):\n",
    "      if dropout:\n",
    "          all_gates_state = tf.matmul(i, ifcox_enc) + tf.matmul(o, ifcom_enc) + ifcob_dec\n",
    "      else:\n",
    "          ifcom_dropout = tf.nn.dropout(ifcom_enc, keep_prob=0.9)\n",
    "          all_gates_state = tf.matmul(i, ifcox_enc) + tf.matmul(o, ifcom_dropout) + ifcob_enc     \n",
    "      input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "      forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "      update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "      state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "      output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "      return output_gate * tf.tanh(state), state\n",
    "    \n",
    "  def decoder_lstm_cell(i, o, state, dropout=False):\n",
    "      if dropout:\n",
    "          all_gates_state = tf.matmul(i, ifcox_dec) + tf.matmul(o, ifcom_dec) + ifcob_dec\n",
    "      else:\n",
    "          ifcom_dropout = tf.nn.dropout(ifcom_dec, keep_prob=0.9)\n",
    "          all_gates_state = tf.matmul(i, ifcox_dec) + tf.matmul(o, ifcom_dropout) + ifcob_dec\n",
    "      input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "      forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "      update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "      state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "      output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "      return output_gate * tf.tanh(state), state\n",
    "    \n",
    "  # Input data\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "      train_data.append(tf.placeholder(tf.float32, shape = [batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data\n",
    "\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings + 1): \n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "  output = tf.constant(0.0, shape=[1,num_nodes], dtype=tf.float32)\n",
    "  state = saved_state\n",
    "    \n",
    "  ## Unrolled encoder LSTM loop\n",
    "  for i in train_inputs:\n",
    "      output, state = encoder_lstm_cell(i, output, state, dropout=False)\n",
    "    \n",
    "  ## Last output from encoder is part of predition\n",
    "  outputs = list([output])\n",
    "\n",
    "  ## Unrolled encoder LSTM loop\n",
    "  for i in train_labels:\n",
    "      output, state = decoder_lstm_cell(i, output, state, dropout=False)\n",
    "      outputs.append(output)\n",
    "\n",
    "\n",
    "##  # State saving across unrollings\n",
    "##  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "##      # Classifier            \n",
    "##      logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "##      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  logits = tf.nn.xw_plus_b(tf.concat(0, outputs[:-1]), w, b)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    1.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  ## TODO: Generate a few validation sentences \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[num_unrollings, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes], dtype=tf.float32))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes], dtype=tf.float32))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1.0, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1.0, num_nodes])))\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output.assign(saved_sample_output),\n",
    "                                saved_sample_state.assign(saved_sample_state)]):\n",
    "    initial_prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, w, b))\n",
    "    for i in range(num_unrollings):\n",
    "      saved_sample_output, saved_sample_state = encoder_lstm_cell(tf.reshape(sample_input[i,:], [1,vocabulary_size]), saved_sample_output, saved_sample_state)\n",
    "      saved_sample_output, saved_sample_state = decoder_lstm_cell(initial_prediction, saved_sample_output, saved_sample_state)\n",
    "      prediction = tf.nn.softmax(tf.nn.xw_plus_b(saved_sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: Y\n",
      "Unexpected character: ,\n",
      "Initialized\n",
      "Average loss at step 0: 3.364308 learning rate: 1.000000\n",
      "================================================================================\n",
      " \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b06cd1f54510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m79\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0msentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "num_steps = 8000\n",
    "summary_frequency = 200\n",
    "\n",
    "validation_sentence_1 = 'You spin my head right round, right round'\n",
    "validation_sentence_2 = 'When you go down, when you go down down'\n",
    "X_validation =  np.zeros(shape=(len(validation_sentence_1), vocabulary_size), dtype=np.float)\n",
    "for b in range(len(validation_sentence_1)):\n",
    "    X_validation[b, char2id(validation_sentence_1[b])] = 1.0\n",
    "    \n",
    "def prob_to_char(probs):\n",
    "    return id2char(np.argmax(probs))\n",
    "    \n",
    "train_batches = BatchGenerator(batch_size, text, num_unrollings)\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches, batches_labels = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i,:,:].T\n",
    "      feed_dict[train_labels[i]] = batches_labels[i,:,:].T\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      #print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, batches_labels.reshape((1344,29))))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        reset_sample_state.run()\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        reset_sample_state.run()\n",
    "        sentence = initial_prediction.eval()\n",
    "        for _ in range(len(validation_sentence_1)):\n",
    "          predictions = prediction.eval({sample_input: feed})\n",
    "          feed = sample(prediction)\n",
    "          sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "        print('=' * 80)      \n",
    "        # TODO: Measure validation set perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1344 / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
